{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch_Learningn (MIVRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ws1.sinaimg.cn/large/abaebc48ly1fqgrsuu355j209x02wjr7.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程7：残差卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码示例：构建一个具有N个residual block 的 残差卷积神经网络对CIFAR10数据集进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10\n",
    "Cifar-10 是由 Hinton 的两个大弟子 Alex Krizhevsky、Ilya Sutskever 收集的一个用于普适物体识别的数据集。Cifar 是加拿大政府牵头投资的一个先进科学项目研究所。Hinton、Bengio和他的学生在2004年拿到了 Cifar 投资的少量资金，建立了神经计算和自适应感知项目。这个项目结集了不少计算机科学家、生物学家、电气工程师、神经科学家、物理学家、心理学家，加速推动了 Deep Learning  的进程。从这个阵容来看，DL 已经和 ML 系的数据挖掘分的很远了。Deep Learning 强调的是自适应感知和人工智能，是计算机与神经科学交叉；Data Mining 强调的是高速、大数据、统计数学分析，是计算机和数学的交叉。\n",
    "\n",
    "Cifar-10 由60000张32*32的 RGB 彩色图片构成，共10个分类。50000张训练，10000张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类（姊妹数据集Cifar-100达到100类，ILSVRC比赛则是1000类）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ws1.sinaimg.cn/large/abaebc48ly1fqiwhpbsuxj20ky0g5k6z.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入需要的库/包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义超参\n",
    "num_epochs = 30\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "block_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#图像预处理\n",
    "\n",
    "# RandomHorizontalFlip 随机水平翻转\n",
    "# RandomCrop 随机截取32*32大小的图片\n",
    "# ToTensor 转换为tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#下载 CIRAR-10 数据集\n",
    "train_dataset = datasets.CIFAR10(root='./data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data/', train=True, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入数据集\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 Residual Block （残差块）\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 把输入保留，后期要用到\n",
    "        input = x\n",
    "        \n",
    "        # 残差学习\n",
    "        residual = self.bn(self.conv1(x))\n",
    "        residual = self.relu(residual)\n",
    "        residual = self.bn(self.conv1(residual))\n",
    "        \n",
    "        # 原始输入和残差相加\n",
    "        out =  input + residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义 Residual Net （残差网络）\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.conv_input = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        \n",
    "        self.residual = self.make_layer(ResidualBlock, block_number)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.avg_pool = nn.AvgPool2d(4)\n",
    "        self.fc = nn.Linear(4096, 10)\n",
    "    \n",
    "    def make_layer(self, block, num):\n",
    "        layers = []\n",
    "        for _ in range(num):\n",
    "            layers.append(block())\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv_input(x)))\n",
    "        \n",
    "        out = self.residual(out)\n",
    "        \n",
    "        out = self.relu(self.bn(self.conv_output(out)))\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        #print(out.size())\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (relu): ReLU()\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv_input): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (residual): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (relu): ReLU()\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (relu): ReLU()\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (2): ResidualBlock(\n",
       "      (relu): ReLU()\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (relu): ReLU()\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (4): ResidualBlock(\n",
       "      (relu): ReLU()\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (conv_output): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear(in_features=4096, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = ResNet()\n",
    "resnet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#定义优化器\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Iter [100/500] Loss: 1.3536\n",
      "Epoch [1/30], Iter [200/500] Loss: 1.2474\n",
      "Epoch [1/30], Iter [300/500] Loss: 1.1035\n",
      "Epoch [1/30], Iter [400/500] Loss: 0.9938\n",
      "Epoch [1/30], Iter [500/500] Loss: 0.9746\n",
      "Epoch [2/30], Iter [100/500] Loss: 0.9522\n",
      "Epoch [2/30], Iter [200/500] Loss: 0.8963\n",
      "Epoch [2/30], Iter [300/500] Loss: 0.6687\n",
      "Epoch [2/30], Iter [400/500] Loss: 0.7480\n",
      "Epoch [2/30], Iter [500/500] Loss: 0.5327\n",
      "Epoch [3/30], Iter [100/500] Loss: 0.9535\n",
      "Epoch [3/30], Iter [200/500] Loss: 0.6597\n",
      "Epoch [3/30], Iter [300/500] Loss: 0.6295\n",
      "Epoch [3/30], Iter [400/500] Loss: 0.7765\n",
      "Epoch [3/30], Iter [500/500] Loss: 0.5492\n",
      "Epoch [4/30], Iter [100/500] Loss: 0.4688\n",
      "Epoch [4/30], Iter [200/500] Loss: 0.7897\n",
      "Epoch [4/30], Iter [300/500] Loss: 0.6606\n",
      "Epoch [4/30], Iter [400/500] Loss: 0.7261\n",
      "Epoch [4/30], Iter [500/500] Loss: 0.5507\n",
      "Epoch [5/30], Iter [100/500] Loss: 0.7363\n",
      "Epoch [5/30], Iter [200/500] Loss: 0.6396\n",
      "Epoch [5/30], Iter [300/500] Loss: 0.5664\n",
      "Epoch [5/30], Iter [400/500] Loss: 0.4638\n",
      "Epoch [5/30], Iter [500/500] Loss: 0.4068\n",
      "Epoch [6/30], Iter [100/500] Loss: 0.4461\n",
      "Epoch [6/30], Iter [200/500] Loss: 0.4479\n",
      "Epoch [6/30], Iter [300/500] Loss: 0.5249\n",
      "Epoch [6/30], Iter [400/500] Loss: 0.4339\n",
      "Epoch [6/30], Iter [500/500] Loss: 0.4499\n",
      "Epoch [7/30], Iter [100/500] Loss: 0.4328\n",
      "Epoch [7/30], Iter [200/500] Loss: 0.5158\n",
      "Epoch [7/30], Iter [300/500] Loss: 0.5507\n",
      "Epoch [7/30], Iter [400/500] Loss: 0.4537\n",
      "Epoch [7/30], Iter [500/500] Loss: 0.5008\n",
      "Epoch [8/30], Iter [100/500] Loss: 0.5089\n",
      "Epoch [8/30], Iter [200/500] Loss: 0.4667\n",
      "Epoch [8/30], Iter [300/500] Loss: 0.6435\n",
      "Epoch [8/30], Iter [400/500] Loss: 0.3502\n",
      "Epoch [8/30], Iter [500/500] Loss: 0.3562\n",
      "Epoch [9/30], Iter [100/500] Loss: 0.3389\n",
      "Epoch [9/30], Iter [200/500] Loss: 0.3862\n",
      "Epoch [9/30], Iter [300/500] Loss: 0.4192\n",
      "Epoch [9/30], Iter [400/500] Loss: 0.4684\n",
      "Epoch [9/30], Iter [500/500] Loss: 0.4426\n",
      "Epoch [10/30], Iter [100/500] Loss: 0.3404\n",
      "Epoch [10/30], Iter [200/500] Loss: 0.3188\n",
      "Epoch [10/30], Iter [300/500] Loss: 0.3014\n",
      "Epoch [10/30], Iter [400/500] Loss: 0.3990\n",
      "Epoch [10/30], Iter [500/500] Loss: 0.4089\n",
      "Epoch [11/30], Iter [100/500] Loss: 0.5323\n",
      "Epoch [11/30], Iter [200/500] Loss: 0.3036\n",
      "Epoch [11/30], Iter [300/500] Loss: 0.2829\n",
      "Epoch [11/30], Iter [400/500] Loss: 0.3808\n",
      "Epoch [11/30], Iter [500/500] Loss: 0.2977\n",
      "Epoch [12/30], Iter [100/500] Loss: 0.4282\n",
      "Epoch [12/30], Iter [200/500] Loss: 0.2800\n",
      "Epoch [12/30], Iter [300/500] Loss: 0.2964\n",
      "Epoch [12/30], Iter [400/500] Loss: 0.2287\n",
      "Epoch [12/30], Iter [500/500] Loss: 0.3221\n",
      "Epoch [13/30], Iter [100/500] Loss: 0.2336\n",
      "Epoch [13/30], Iter [200/500] Loss: 0.2507\n",
      "Epoch [13/30], Iter [300/500] Loss: 0.3901\n",
      "Epoch [13/30], Iter [400/500] Loss: 0.4141\n",
      "Epoch [13/30], Iter [500/500] Loss: 0.3707\n",
      "Epoch [14/30], Iter [100/500] Loss: 0.2282\n",
      "Epoch [14/30], Iter [200/500] Loss: 0.3100\n",
      "Epoch [14/30], Iter [300/500] Loss: 0.2658\n",
      "Epoch [14/30], Iter [400/500] Loss: 0.3448\n",
      "Epoch [14/30], Iter [500/500] Loss: 0.3300\n",
      "Epoch [15/30], Iter [100/500] Loss: 0.2911\n",
      "Epoch [15/30], Iter [200/500] Loss: 0.3597\n",
      "Epoch [15/30], Iter [300/500] Loss: 0.3025\n",
      "Epoch [15/30], Iter [400/500] Loss: 0.2802\n",
      "Epoch [15/30], Iter [500/500] Loss: 0.3883\n",
      "Epoch [16/30], Iter [100/500] Loss: 0.3544\n",
      "Epoch [16/30], Iter [200/500] Loss: 0.3528\n",
      "Epoch [16/30], Iter [300/500] Loss: 0.3869\n",
      "Epoch [16/30], Iter [400/500] Loss: 0.2405\n",
      "Epoch [16/30], Iter [500/500] Loss: 0.2526\n",
      "Epoch [17/30], Iter [100/500] Loss: 0.2473\n",
      "Epoch [17/30], Iter [200/500] Loss: 0.2044\n",
      "Epoch [17/30], Iter [300/500] Loss: 0.2281\n",
      "Epoch [17/30], Iter [400/500] Loss: 0.1915\n",
      "Epoch [17/30], Iter [500/500] Loss: 0.2345\n",
      "Epoch [18/30], Iter [100/500] Loss: 0.2269\n",
      "Epoch [18/30], Iter [200/500] Loss: 0.1730\n",
      "Epoch [18/30], Iter [300/500] Loss: 0.2315\n",
      "Epoch [18/30], Iter [400/500] Loss: 0.3454\n",
      "Epoch [18/30], Iter [500/500] Loss: 0.3046\n",
      "Epoch [19/30], Iter [100/500] Loss: 0.1199\n",
      "Epoch [19/30], Iter [200/500] Loss: 0.1309\n",
      "Epoch [19/30], Iter [300/500] Loss: 0.2152\n",
      "Epoch [19/30], Iter [400/500] Loss: 0.1392\n",
      "Epoch [19/30], Iter [500/500] Loss: 0.3568\n",
      "Epoch [20/30], Iter [100/500] Loss: 0.2302\n",
      "Epoch [20/30], Iter [200/500] Loss: 0.1964\n",
      "Epoch [20/30], Iter [300/500] Loss: 0.2373\n",
      "Epoch [20/30], Iter [400/500] Loss: 0.2014\n",
      "Epoch [20/30], Iter [500/500] Loss: 0.3780\n",
      "Epoch [21/30], Iter [100/500] Loss: 0.1706\n",
      "Epoch [21/30], Iter [200/500] Loss: 0.1303\n",
      "Epoch [21/30], Iter [300/500] Loss: 0.1649\n",
      "Epoch [21/30], Iter [400/500] Loss: 0.1769\n",
      "Epoch [21/30], Iter [500/500] Loss: 0.1864\n",
      "Epoch [22/30], Iter [100/500] Loss: 0.2112\n",
      "Epoch [22/30], Iter [200/500] Loss: 0.1885\n",
      "Epoch [22/30], Iter [300/500] Loss: 0.2589\n",
      "Epoch [22/30], Iter [400/500] Loss: 0.2580\n",
      "Epoch [22/30], Iter [500/500] Loss: 0.1426\n",
      "Epoch [23/30], Iter [100/500] Loss: 0.1022\n",
      "Epoch [23/30], Iter [200/500] Loss: 0.1178\n",
      "Epoch [23/30], Iter [300/500] Loss: 0.1544\n",
      "Epoch [23/30], Iter [400/500] Loss: 0.1964\n",
      "Epoch [23/30], Iter [500/500] Loss: 0.1425\n",
      "Epoch [24/30], Iter [100/500] Loss: 0.1949\n",
      "Epoch [24/30], Iter [200/500] Loss: 0.1316\n",
      "Epoch [24/30], Iter [300/500] Loss: 0.2051\n",
      "Epoch [24/30], Iter [400/500] Loss: 0.0847\n",
      "Epoch [24/30], Iter [500/500] Loss: 0.0728\n",
      "Epoch [25/30], Iter [100/500] Loss: 0.1411\n",
      "Epoch [25/30], Iter [200/500] Loss: 0.2866\n",
      "Epoch [25/30], Iter [300/500] Loss: 0.1795\n",
      "Epoch [25/30], Iter [400/500] Loss: 0.1276\n",
      "Epoch [25/30], Iter [500/500] Loss: 0.2355\n",
      "Epoch [26/30], Iter [100/500] Loss: 0.1108\n",
      "Epoch [26/30], Iter [200/500] Loss: 0.1500\n",
      "Epoch [26/30], Iter [300/500] Loss: 0.1400\n",
      "Epoch [26/30], Iter [400/500] Loss: 0.1747\n",
      "Epoch [26/30], Iter [500/500] Loss: 0.1628\n",
      "Epoch [27/30], Iter [100/500] Loss: 0.2075\n",
      "Epoch [27/30], Iter [200/500] Loss: 0.1592\n",
      "Epoch [27/30], Iter [300/500] Loss: 0.1548\n",
      "Epoch [27/30], Iter [400/500] Loss: 0.1521\n",
      "Epoch [27/30], Iter [500/500] Loss: 0.0626\n",
      "Epoch [28/30], Iter [100/500] Loss: 0.0912\n",
      "Epoch [28/30], Iter [200/500] Loss: 0.1452\n",
      "Epoch [28/30], Iter [300/500] Loss: 0.1077\n",
      "Epoch [28/30], Iter [400/500] Loss: 0.0705\n",
      "Epoch [28/30], Iter [500/500] Loss: 0.1516\n",
      "Epoch [29/30], Iter [100/500] Loss: 0.0692\n",
      "Epoch [29/30], Iter [200/500] Loss: 0.1197\n",
      "Epoch [29/30], Iter [300/500] Loss: 0.0872\n",
      "Epoch [29/30], Iter [400/500] Loss: 0.1585\n",
      "Epoch [29/30], Iter [500/500] Loss: 0.1257\n",
      "Epoch [30/30], Iter [100/500] Loss: 0.0797\n",
      "Epoch [30/30], Iter [200/500] Loss: 0.0787\n",
      "Epoch [30/30], Iter [300/500] Loss: 0.0578\n",
      "Epoch [30/30], Iter [400/500] Loss: 0.0783\n",
      "Epoch [30/30], Iter [500/500] Loss: 0.1753\n"
     ]
    }
   ],
   "source": [
    "#训练网络\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "        \n",
    "        #调用残差网络进行前向传播\n",
    "        outputs = resnet(images)\n",
    "        \n",
    "        #调用损失函数，求输出值与真实值之间的差\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #启用优化器并将梯度缓存清空，再进行反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        #调用优化函数开始迭代优化\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [%d/%d], Iter [%d/%d] Loss: %.4f\" %(epoch+1, num_epochs, i+1, 500, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "#网络测试\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.cuda())\n",
    "    outputs = resnet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "    \n",
    "print('Accuracy of the model on the test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
